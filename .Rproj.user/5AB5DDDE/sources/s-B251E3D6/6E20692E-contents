---
title: "RandomForest"
author: "Michael Schmidt"
date: "February 20, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Step 1 Load libraries
```{r}
library(tidyverse)
library(randomForest)
library(caret)
```
## Step 2 - read in the data
```{r}
data<-readRDS("outputs/final_dataset/training_dataset.rds")
```

## Step 3 - Subset data only arch sites are create predictive column
```{r}
training_dataset<-data%>%
  mutate(training_area=ifelse(!is.na(arch_survey) , 1, 0))%>%
  filter(training_area==1)%>%
  mutate(arch_site_present=ifelse(!is.na(arch_sites), 1, 0))%>%
  select(-arch_sites, -training_area, -arch_survey)%>%
  drop_na()
```

## Step 4 - Subset data into training and testing for model run and subsequent validation. 
```{r}
set.seed(415)
train<-sample_frac(training_dataset, 0.7)
sid<-as.numeric(rownames(train))
test<-training_dataset[-sid,]
rm(sid)

```


## Step 5.Option1 - Run Random Forest
```{r}
nmin <- min(table(training_dataset$arch_site_present))

ptm<- proc.time()

fit<-randomForest(as.factor(arch_site_present)~ . , 
                  data=training_dataset,
                  importance=TRUE,
                  sampsize=c(nmin,nmin), ##This, sampsize, is extremely important. Random forests performs poorly with uneven classes (ie class 1 has a count of 500 and class 2 has a cound of 10,000).  We need to even out the sample sizes in the model.  How this is by taking the value with the lowest count and setting the second value to same or different proportion, in this case we only have 1029 observations that have candy.  If you wanted an evenly weighted sample size/weight you would set the sampsize to `sampsize=c(1029,1029)`.  In this case we want to overpredict candy so we give it a greater weight than the non candy sites. 
                  mtry=11,
                  ntree=500, 
                  do.trace=50)
proc.time() - ptm
varImpPlot(fit)
print(fit)
importance(fit)
```

## Step 5.1.Option1 - Save Model
```{r}
saveRDS(fit, "outputs/model_runs/fit_500_training_set_all_03072019.rds")
```

## Step 5.Option2 - Run Random Forest with Caret for Tuning
This takes a long, long time. 
```{r}
control <- trainControl(method = "repeatedcv", 
                     number = 5, 
                     repeats = 5, 
                     verboseIter = FALSE,
                     sampling="up"
                     )

tunegrid <- expand.grid(.mtry=c(1:15))

ptm<- proc.time()

caret_fit_base<-train(as.factor(arch_site_present)~ . , 
                  data=training_dataset,
                  method="rf",
                  metric="Accuracy",
                  preProcess = c("scale", "center"),
                  trControl=control
                  )
proc.time() - ptm

```
## Step 5.2.Option2 - Save Caret Model
```{r}
saveRDS(caret, "output/fit_models/caret_model_INCLUDE_CANM_2222019.rds")
```


## Step 6 - Validation
The following applies the model `fit` to the `test` data and then determines how well it predicted both the candy sites and the non-cany sites. 

```{r}

prediction_test<- transform(test, predict=predict(fit, test))%>%
  mutate(success=if_else(arch_site_present==predict, 1, 0))

arc_predict<-filter(prediction_test, arch_site_present==1)
no_predict<-filter(prediction_test, arch_site_present==0)

count_arc_predict<- table(test$arch_site_present)
count_arc_predict

sum(arc_predict$success)/count_arc_predict[names(count_arc_predict)==1]*100 
sum(no_predict$success)/count_arc_predict[names(count_arc_predict)==0]*100

```

